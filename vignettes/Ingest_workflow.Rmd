---
title: "RASSLER ingest workflow"
description: > 
  How to use RASSLER to ingest ras models into a catalog and not lose your mind.
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{RASSLER ingest workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

> RASSLER expects to operate within it's own controlled directory, or "ras_model_catalog".   Users first download and unpack desired models into a temporary location, and then point RRASSLER at that directory and the place which you are placing your catalog. 

This is the heart and soul of the RRASSLER workflow, so it's worth unpacking this.  To start out with an example, lets demonstrate how you might use RRASSLER to ingest FEMA region 6 BLE data.  While we could just donwload one of those files, lets make life harder and write a quick scaper:
```{r}
url_exists <- function(x, non_2xx_return_value = FALSE, quiet = FALSE,...) {
  
  suppressPackageStartupMessages({ require("httr", quietly = FALSE, warn.conflicts = FALSE) })
  capture_error <- function(code, otherwise = NULL, quiet = TRUE) {
    tryCatch(
      list(result = code, error = NULL),
      error = function(e) {
        if (!quiet)
          message("Error: ", e$message)
        
        list(result = otherwise, error = e)
      },
      interrupt = function(e) {
        stop("Terminated by user", call. = FALSE)
      }
    )
  }
  
  safely <- function(.f, otherwise = NULL, quiet = TRUE) {
    function(...) capture_error(.f(...), otherwise, quiet)
  }
  
  sHEAD <- safely(httr::HEAD)
  sGET <- safely(httr::GET)
  
  # Try HEAD first since it's lightweight
  res <- sHEAD(x, ...)
  
  if (is.null(res$result) ||
      ((httr::status_code(res$result) %/% 200) != 1)) {
    
    res <- sGET(x, ...)
    
    if (is.null(res$result)) return(NA) # or whatever you want to return on "hard" errors
    
    if (((httr::status_code(res$result) %/% 200) != 1)) {
      if (!quiet) warning(sprintf("Requests for [%s] responded but without an HTTP status code in the 200-299 range", x))
      return(non_2xx_return_value)
    }
    
    return(TRUE)
    
  } else {
    return(TRUE)
  }
  
}
scrape_ble_lib <- function(database_path=NULL,HUCID,quiet=FALSE,overwrite=FALSE) {
  fn_time_start <- Sys.time()
  
  output_dir <- file.path(database_path,HUCID, fsep=.Platform$file.sep)
  if(file.exists(output_dir)){
    if(overwrite) {
      unlink(output_dir, recursive=TRUE)
    } else {
      fn_warning_block()
      print("file downloaded and overwrite is set to FALSE")
      return(FALSE)
    }
  }
  
  if(file.exists(file.path("J:/Dropbox/root/database/hosted/water/HUC8.fgb",fsep=.Platform$file.sep))) {
    template_hucs <- sf::st_transform(sf::st_read(file.path("J:/Dropbox/root/database/hosted/water/HUC8.fgb",fsep=.Platform$file.sep),quiet=FALSE),sf::st_crs("EPSG:5070"))
  } else {
    template_hucs <- sf::st_transform(sf::st_read('https://waterduck.ddns.net:9000/water/HUC8.fgb',quiet=FALSE),sf::st_crs("EPSG:5070"))
  }  
  Potential_features <- template_hucs[template_hucs$huc8 == HUCID,]
  
  if(nrow(Potential_features)==0) {
    fn_error_block()
    print("No features found")
    return(FALSE)
  }
  Potential_features$SpatialData_url =
    paste0("https://ebfedata.s3.amazonaws.com/",Potential_features$huc8,"_",gsub(" ","",gsub("-","",Potential_features$name,fixed = TRUE),fixed = TRUE),"/",Potential_features$huc8,"_SpatialData.zip")
  Potential_features$RASData_url =
    paste0("https://ebfedata.s3.amazonaws.com/",Potential_features$huc8,"_",gsub(" ","",gsub("-","",Potential_features$name,fixed = TRUE),fixed = TRUE),"/",Potential_features$huc8,"_Models.zip")
  Potential_features$Reports_url =
    paste0("https://ebfedata.s3.amazonaws.com/",Potential_features$huc8,"_",gsub(" ","",gsub("-","",Potential_features$name,fixed = TRUE),fixed = TRUE),"/",Potential_features$huc8,"_Documents.zip")
  
  if(url_exists(Potential_features$SpatialData_url)) {
    if (!dir.exists(output_dir)) { dir.create(output_dir) }
    httr::GET(Potential_features$SpatialData_url,
              httr::write_disk(file.path(output_dir,basename(Potential_features$SpatialData_url), fsep=.Platform$file.sep), overwrite=TRUE),
              overwrite=TRUE)
    httr::GET(Potential_features$RASData_url,
              httr::write_disk(file.path(output_dir,basename(Potential_features$RASData_url), fsep=.Platform$file.sep), overwrite=TRUE),
              overwrite=TRUE)
    httr::GET(Potential_features$Reports_url,
              httr::write_disk(file.path(output_dir,basename(Potential_features$Reports_url), fsep=.Platform$file.sep), overwrite=TRUE),
              overwrite=TRUE)
    
    if(!quiet) {
      disk_size <- round(sum(file.info(list.files(output_dir,full.names=TRUE,recursive = TRUE))$size) * 1e-9,3)
      print(glue::glue("Scraped {disk_size} GB in {round(difftime(Sys.time(), fn_time_start, units='mins'), digits = 2)} minutes"))
    }
    return(TRUE)
  } else {
    if(!quiet) {
      fn_warning_block()
      print("URL was not found, tested values:")
      print(Potential_features$SpatialData_url)
    }
    return(FALSE)
  }
}
scrape_ble_lib("J:/data/BLE/fema/","12090301")
```

Next we need to unpack that data

```{r}
unzipZipfiles <- function(zippath,quiet=FALSE) {
  fn_time_start <- Sys.time()
  
  if(stringr::str_sub(zippath,-3,-1) == "zip") {
    utils::unzip(zippath, exdir = file.path(dirname(zippath),gsub('.{4}$', '',basename(zippath)),fsep = .Platform$file.sep))
    zippath <- file.path(dirname(zippath),gsub('.{4}$', '',basename(zippath)),fsep = .Platform$file.sep)
  }
  list_of_processed_zips <- c()
  files_to_process <- list.files(zippath, pattern=glob2rx(glue::glue("*.zip$")), full.names=TRUE, ignore.case=TRUE, recursive=TRUE)
  i = 1
  while(length(files_to_process) > 0) {
    print(glue::glue("Unzipping iteration: {i} - Files processed: {length(list_of_processed_zips)}"))
    for(zip_file in files_to_process) {
      utils::unzip(zip_file, exdir = file.path(dirname(zip_file),gsub('.{4}$', '',basename(zip_file)),fsep = .Platform$file.sep))
    }
    list_of_processed_zips <- append(list_of_processed_zips,files_to_process)
    list_of_all_zips <- list.files(zippath, pattern=glob2rx(glue::glue("*.zip$")), full.names=TRUE, ignore.case=TRUE, recursive=TRUE)
    files_to_process <- dplyr::setdiff(list_of_all_zips,list_of_processed_zips)
    i <- i+1
  }
  disk_size <- round(sum(file.info(list.files(zippath,full.names=TRUE,recursive = TRUE))$size) * 1e-9,3)
  if(!quiet) {
    disk_size <- round(sum(file.info(list.files(zippath,full.names=TRUE,recursive = TRUE))$size) * 1e-9,3)
    print(glue::glue("Finished: Unzipped {length(list_of_processed_zips)} files with a disk size of ~{disk_size} GB in {round(difftime(Sys.time(), fn_time_start, units='mins'), digits = 2)} minutes"))
  }
  return(TRUE)
}
unzipZipfiles("J:/data/BLE/fema/08080101/08080101_Models")
```

Now we can start the ingest process.

```{r}
devtools::install_github(JimColl/RRASSLER)

RRASSLER::ingest_into_database(path_to_ras_dbase="J:/data/ras_dbase",
                     top_of_dir_to_scrape="J:/data/BLE/fema/12090301/12090301_Models",
                     code_to_place_in_source="FEMA R6",
                     proj_overwrite="EPSG:2277",
                     vdat_trans=TRUE,
                     quiet=TRUE,
                     overwrite=TRUE,
                     refresh=TRUE) 
```
